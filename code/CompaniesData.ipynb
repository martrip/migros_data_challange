{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d13c46-528b-43f4-8cda-f4bb47c4d507",
   "metadata": {},
   "source": [
    "# Migros Location - Number of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43696d5e-f6c9-4119-b29f-adba802adfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the robots.txt: https://www.swissyello.com/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e285a5b-097b-4b84-b3ba-4f67de4f195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"example app\")\n",
    "# https://towardsdatascience.com/pythons-geocoding-convert-a-list-of-addresses-into-a-map-f522ef513fd6#5352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71628b-db38-4724-9bfb-6d19624689f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction functions definition\n",
    "\n",
    "# Field: Company name\n",
    "def extr_company_name(element):\n",
    "    try:\n",
    "        company_name = element.find(\"h4\").get_text()\n",
    "    except:\n",
    "        company_name = \"\"\n",
    "        \n",
    "    return company_name\n",
    "\n",
    "\n",
    "# Field: Company address\n",
    "def extr_company_address(element):\n",
    "    try:\n",
    "        company_address = element.find(\"div\", {\"class\": \"address\"}).get_text()\n",
    "    except:\n",
    "        company_address = \"\"\n",
    "    \n",
    "    return company_address\n",
    "\n",
    "\n",
    "# Field: Company latitude\n",
    "def extr_company_ltd(element):\n",
    "    try:\n",
    "        company_ltd = element.find(\"a\", {\"class\" : \"mapmarker\"}).get(\"data-ltd\")\n",
    "    except:\n",
    "        company_ltd = \"\"\n",
    "    \n",
    "    return company_ltd\n",
    "\n",
    "\n",
    "# Field: Company longitude\n",
    "def extr_company_lng(element):\n",
    "    try:\n",
    "        company_lng = element.find(\"a\", {\"class\" : \"mapmarker\"}).get(\"data-lng\")\n",
    "    except:\n",
    "        company_lng = \"\"\n",
    "    \n",
    "    return company_lng\n",
    "\n",
    "\n",
    "# Field: Company verified\n",
    "def extr_company_verified(element):\n",
    "    try:\n",
    "        company_verified = element.find(\"u\", {\"class\": \"v\"}).get_text()\n",
    "    except:\n",
    "        company_verified = \"\"\n",
    "    \n",
    "    return company_verified\n",
    "\n",
    "\n",
    "# Field: Company coordinates\n",
    "def extr_company_coordinates(address):\n",
    "    \n",
    "        try:\n",
    "            data = geolocator.geocode(company_address)\n",
    "            company_coordinates = data.point\n",
    "        except:\n",
    "            company_coordinates = \"\"\n",
    "            \n",
    "        return company_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df74ee-208a-430a-84d3-d81b161ac9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the objects needed for the scrape\n",
    "url_base = \"https://www.swissyello.com/location/Zurich/\"\n",
    "urls_failed = []\n",
    "logs_dict = {}\n",
    "logs_df = pd.DataFrame()\n",
    "swissyello_df = pd.DataFrame()\n",
    "\n",
    "for page_num in range(501, 600):\n",
    "    \n",
    "    # Create the url to scrape based on the page number\n",
    "    url_scrape = url_base + str(page_num)\n",
    "    \n",
    "    # Request the page and load the response\n",
    "    try:\n",
    "        resp = requests.get(url_scrape) # , timeout=1\n",
    "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    except:\n",
    "        print(\"url get error\")\n",
    "        urls_failed.append(url_scrape)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Create a dict to append the data to a df\n",
    "    swissyello_dict = {}\n",
    "    \n",
    "    # Download data of all the jobs in each page\n",
    "    for element in soup.findAll(\"div\", {\"class\": \"company g_0\"}):\n",
    "        \n",
    "        # Extract each field\n",
    "        company_name     = extr_company_name(element)\n",
    "        company_address  = extr_company_address(element)\n",
    "        company_ltd      = extr_company_ltd(element)\n",
    "        company_lng      = extr_company_lng(element)\n",
    "        company_verified = extr_company_verified(element)\n",
    "        \n",
    "        # Extract coordinates from an address\n",
    "        company_coordinates = extr_company_coordinates(company_address)\n",
    "\n",
    "        swissyello_dict = {\"company_name\"        : company_name,\n",
    "                           \"company_address\"     : company_address,\n",
    "                           \"company_ltd\"         : company_ltd,\n",
    "                           \"company_lng\"         : company_lng,\n",
    "                           \"company_verified\"    : company_verified,\n",
    "                           \"company_coordinates\" : company_coordinates\n",
    "                          }\n",
    "        \n",
    "        # Insert rows in the dataframe\n",
    "        swissyello_df = swissyello_df.append(swissyello_dict, True)\n",
    "    \n",
    "    # Register the logs in case of failure\n",
    "    logs_dict = {\"url\": url_scrape}\n",
    "    logs_df = logs_df.append(logs_dict, True)\n",
    "    \n",
    "    if page_num % 10 == 0:\n",
    "        print(\"{}: OK\".format(page_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141f194-0e68-49b5-84da-d76189ae4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "swissyello_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba67c2-9600-4be1-b8d1-084cf113524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "swissyello_df.to_csv(\"companies_raw_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206dc11-fb9d-4b9a-a943-e9483b6d9063",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa5f7c3-b0e7-4de4-bba0-35dec5f588e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'point'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10748/3193149608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcompany_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcompany_coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'point'"
     ]
    }
   ],
   "source": [
    "company_address = \"Schaffhauserstrasse 333, P.O. Box: 8050, Zurich\t\"\n",
    "data = geolocator.geocode(company_address)\n",
    "company_coordinates = data.point\n",
    "company_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42e826-328d-410f-979b-0d5ff1832601",
   "metadata": {},
   "source": [
    "# WebScraping Glassdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07220570-3e87-45a8-be71-0ff0c36ec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce810c5-3009-4ff2-b7c2-5d260472eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion",
   "language": "python",
   "name": "propulsion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
